{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saiprakashlikky/Desktop/Projects/ML_projects/Flight_Fare_estimator_Project/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saiprakashlikky/Desktop/Projects/ML_projects/Flight_Fare_estimator_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataModellingConfig:\n",
    "    root_dir: Path\n",
    "    x_datapath: Path\n",
    "    y_datapath: Path\n",
    "    max_depth: float\n",
    "    max_features: str\n",
    "    min_samples_leaf: float\n",
    "    min_samples_split: float\n",
    "    n_estimators: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Flight_Fare_estimator_Project.constants import *\n",
    "from src.Flight_Fare_estimator_Project.utils.common import read_yaml, create_directories\n",
    "from src.Flight_Fare_estimator_Project.pipeline.stage_3_data_transformation import DataTransformation_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_modelling_config(self) -> DataModellingConfig:\n",
    "        config = self.config.data_modelling\n",
    "        params= self.params.xgboost_params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        datamodelling_config = DataModellingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            x_datapath=config.x_datapath,\n",
    "            y_datapath=config.y_datapath,\n",
    "            max_depth=params.max_depth,\n",
    "            max_features=params.max_features,\n",
    "            min_samples_leaf=params.min_samples_leaf,\n",
    "            min_samples_split=params.min_samples_split,\n",
    "            n_estimators=params.n_estimators\n",
    "        )\n",
    "\n",
    "        return datamodelling_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from src.Flight_Fare_estimator_Project import logger\n",
    "from src.Flight_Fare_estimator_Project.utils.common import get_size\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "import os\n",
    "from src.Flight_Fare_estimator_Project import logger\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModelling:\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "    \n",
    "\n",
    "    def get_data_transformed_object(self,file_path):\n",
    "        \"\"\"\n",
    "        Method: To handle categorical vaiables and standardization\n",
    "        Description: This method is used to standardize the data and handle categorical variables.\n",
    "        Parameters: Outfile file path name\n",
    "        Return: preprocessor_obj, Scaled Independent Features and output feature\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            x=pd.read_csv(self.config.x_datapath)\n",
    "            y=pd.read_csv(self.config.y_datapath)\n",
    "\n",
    "            numerical_columns = ['Duration_minutes']\n",
    "            categorical_columns = ['Airline', 'Source', 'Destination', 'Total_Stops']\n",
    "\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"one_hot_encoder\", OneHotEncoder()),\n",
    "                    (\"scaler\", StandardScaler(with_mean=False))\n",
    "                ]\n",
    "            )\n",
    "            logger.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logger.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "                    (\"cat_pipelines\", cat_pipeline, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            scaled_x = preprocessor.fit_transform(x)\n",
    "            pickle_filepath = os.path.join(self.config.root_dir, file_path)\n",
    "            with open(pickle_filepath, 'wb') as file:\n",
    "                pickle.dump(scaled_x, file)\n",
    "\n",
    "            return preprocessor,scaled_x,y\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def train_test_variables(self, x_scaled_variable,y):\n",
    "        \"\"\"\n",
    "        Method: Extracting Train and Test Variables\n",
    "        Description: This method is used to extract Train and test variables from Dataframe.\n",
    "        Parameters: dependent and Independent variable\n",
    "        Return: dependent and Independent variables after Train test Split\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            logger.info(f\"Shape of data is X: {x_scaled_variable.shape},Y:{y.shape}\")\n",
    "\n",
    "\n",
    "            logger.info(\"Train Test Split of The Data Started\")\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_scaled_variable, y, test_size=0.2, random_state=42)\n",
    "            logger.info(f\"Train Test Split Completed. Shapes of training and test data:\\n{x_train.shape}\\n{x_test.shape}\\n{y_train.shape}\\n{y_test.shape}\")\n",
    "            return x_train, x_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def model_trainer(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Method: Training the model from getting best models after research test\n",
    "        Description: This method is used create best model.\n",
    "        Parameters: train and test variables\n",
    "        Return: model file\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Training XGBoost Model Started\")\n",
    "\n",
    "            # Access XGBoost parameters from the configuration\n",
    "            xgb_params = {\n",
    "                'max_depth': self.config.max_depth,\n",
    "                'max_features': self.config.max_features,\n",
    "                'min_samples_leaf': self.config.min_samples_leaf,\n",
    "                'min_samples_split': self.config.min_samples_split,\n",
    "                'n_estimators': self.config.n_estimators\n",
    "            }\n",
    "\n",
    "            xgb_model = XGBRegressor(**xgb_params)\n",
    "            xgb_model.fit(x_train, y_train)\n",
    "            logger.info(\"Training XGBoost Model Completed\")\n",
    "\n",
    "            xgb_model_filepath = os.path.join(self.config.root_dir, 'xgb_model.pkl')\n",
    "            with open(xgb_model_filepath, 'wb') as file:\n",
    "                pickle.dump(xgb_model, file)\n",
    "\n",
    "            logger.info(f\"XGBoost model saved to {xgb_model_filepath}\")\n",
    "            return xgb_model\n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-28 00:57:31,363:INFO:common:yaml file: config/config.yaml loaded successfully]\n",
      "[2023-11-28 00:57:31,366:INFO:common:yaml file: params.yaml loaded successfully]\n",
      "[2023-11-28 00:57:31,367:INFO:common:yaml file: schema.yaml loaded successfully]\n",
      "[2023-11-28 00:57:31,368:INFO:common:created directory at: artifacts]\n",
      "[2023-11-28 00:57:31,368:INFO:common:created directory at: artifacts/data_modelling]\n",
      "[2023-11-28 00:57:31,382:INFO:1684455004:Categorical columns: ['Airline', 'Source', 'Destination', 'Total_Stops']]\n",
      "[2023-11-28 00:57:31,382:INFO:1684455004:Numerical columns: ['Duration_minutes']]\n",
      "[2023-11-28 00:57:31,400:INFO:1684455004:Shape of data is X: (10682, 29),Y:(10682, 1)]\n",
      "[2023-11-28 00:57:31,401:INFO:1684455004:Train Test Split of The Data Started]\n",
      "[2023-11-28 00:57:31,404:INFO:1684455004:Train Test Split Completed. Shapes of training and test data:\n",
      "(8545, 29)\n",
      "(2137, 29)\n",
      "(8545, 1)\n",
      "(2137, 1)]\n",
      "[2023-11-28 00:57:31,404:INFO:1684455004:Training XGBoost Model Started]\n",
      "[2023-11-28 00:57:31,483:INFO:1684455004:Training XGBoost Model Completed]\n",
      "[2023-11-28 00:57:31,485:INFO:1684455004:XGBoost model saved to artifacts/data_modelling/xgb_model.pkl]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiprakashlikky/anaconda3/envs/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [00:57:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_modelling_config = config.get_modelling_config()\n",
    "    data_modelling = DataModelling(config=data_modelling_config)\n",
    "    preprocessor_obj, x_scaled, y = data_modelling.get_data_transformed_object(\"scaler.pkl\")\n",
    "    x_train, x_test, y_train, y_test = data_modelling.train_test_variables(x_scaled, y)\n",
    "    xgb_model = data_modelling.model_trainer(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
