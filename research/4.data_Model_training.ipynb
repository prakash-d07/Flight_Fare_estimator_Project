{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saiprakashlikky/Desktop/Projects/ML_projects/Flight_Fare_estimator_Project/research'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saiprakashlikky/Desktop/Projects/ML_projects/Flight_Fare_estimator_Project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataModellingConfig:\n",
    "    root_dir: Path\n",
    "    x_datapath: Path\n",
    "    y_datapath: Path\n",
    "    model_file_path: Path\n",
    "    preprocessor_file_path: Path\n",
    "    max_depth: float\n",
    "    max_features: str\n",
    "    min_samples_leaf: float\n",
    "    min_samples_split: float\n",
    "    n_estimators: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Flight_Fare_estimator_Project.constants import *\n",
    "from src.Flight_Fare_estimator_Project.utils.common import read_yaml, create_directories,save_json\n",
    "from src.Flight_Fare_estimator_Project.pipeline.stage_3_data_transformation import DataTransformation_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_modelling_config(self) -> DataModellingConfig:\n",
    "        config = self.config.data_modelling\n",
    "        params= self.params.xgboost_params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        datamodelling_config = DataModellingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            x_datapath=config.x_datapath,\n",
    "            y_datapath=config.y_datapath,\n",
    "            model_file_path=config.model_file_path,\n",
    "            preprocessor_file_path= config.preprocessor_file_path,\n",
    "            max_depth=params.max_depth,\n",
    "            max_features=params.max_features,\n",
    "            min_samples_leaf=params.min_samples_leaf,\n",
    "            min_samples_split=params.min_samples_split,\n",
    "            n_estimators=params.n_estimators\n",
    "        )\n",
    "\n",
    "        return datamodelling_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from src.Flight_Fare_estimator_Project import logger\n",
    "from src.Flight_Fare_estimator_Project.utils.common import get_size\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "import os\n",
    "from src.Flight_Fare_estimator_Project import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModelling:\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "    \n",
    "\n",
    "    def get_data_transformed_object(self):\n",
    "        try:\n",
    "            x = pd.read_csv(self.config.x_datapath)\n",
    "            y = pd.read_csv(self.config.y_datapath)\n",
    "\n",
    "            logger.info(f\"Columns in x: {x.columns}\")\n",
    "            logger.info(f\"{x.dtypes}\")\n",
    "\n",
    "            numerical_columns = ['Duration_minutes', 'Month_of_Month_of_journey', 'day_of_date_of_journey']\n",
    "            categorical_columns = ['Airline', 'Source', 'Destination', 'Total_Stops']\n",
    "\n",
    "            data_OHE = pd.concat([x[['Month_of_Month_of_journey', 'day_of_date_of_journey', 'Duration_minutes']],\n",
    "                      pd.get_dummies(x.Airline),   \n",
    "                      pd.get_dummies(x.Source, prefix='source'),  \n",
    "                      pd.get_dummies(x.Destination, prefix='destination'), \n",
    "                      pd.get_dummies(x.Total_Stops)], \n",
    "                    axis=1)\n",
    "            data_OHE = data_OHE.astype(int)\n",
    "            numerical_data = data_OHE[numerical_columns]\n",
    "            scaler = StandardScaler()\n",
    "            scaled_numerical_data = scaler.fit_transform(numerical_data)\n",
    "            scaled_numerical_df = pd.DataFrame(scaled_numerical_data, columns=numerical_columns)\n",
    "            scaled_numerical_df.reset_index(drop=True, inplace=True)\n",
    "            data_OHE.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "            data_OHE.drop(columns=numerical_columns, inplace=True)\n",
    "\n",
    "            scaled_data_OHE = pd.concat([scaled_numerical_df, data_OHE], axis=1)\n",
    "\n",
    "            scaler_filename = self.config.preprocessor_file_path\n",
    "            with open(scaler_filename, 'wb') as scaler_file:\n",
    "                pickle.dump(scaler, scaler_file)\n",
    "\n",
    "            logger.info(f\"{scaled_data_OHE.shape}\")\n",
    "\n",
    "            return scaled_data_OHE, y\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def train_test_variables(self, x_scaled_variable, y):\n",
    "        try:\n",
    "            logger.info(f\"Shape of data is X: {x_scaled_variable.shape},Y:{y.shape}\")\n",
    "\n",
    "            logger.info(\"Train Test Split of The Data Started\")\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_scaled_variable, y, test_size=0.2, random_state=42)\n",
    "            logger.info(f\"Train Test Split Completed. Shapes of training and test data:\\n{x_train.shape}\\n{x_test.shape}\\n{y_train.shape}\\n{y_test.shape}\")\n",
    "            return x_train, x_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def model_trainer(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Method: Training the model from getting best models after research test\n",
    "        Description: This method is used create best model.\n",
    "        Parameters: train and test variables\n",
    "        Return: model file\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Training XGBoost Model Started\")\n",
    "\n",
    "            # Access XGBoost parameters from the configuration\n",
    "            xgb_params = {\n",
    "                'max_depth': self.config.max_depth,\n",
    "                'max_features': self.config.max_features,\n",
    "                'min_samples_leaf': self.config.min_samples_leaf,\n",
    "                'min_samples_split': self.config.min_samples_split,\n",
    "                'n_estimators': self.config.n_estimators\n",
    "            }\n",
    "\n",
    "            xgb_model = XGBRegressor(**xgb_params)\n",
    "            xgb_model.fit(x_train, y_train)\n",
    "            logger.info(\"Training XGBoost Model Completed\")\n",
    "\n",
    "            xgb_model_filepath = os.path.join(self.config.root_dir, 'xgb_model.pkl')\n",
    "            with open(xgb_model_filepath, 'wb') as file:\n",
    "                pickle.dump(xgb_model, file)\n",
    "\n",
    "            logger.info(f\"XGBoost model saved to {xgb_model_filepath}\")\n",
    "            return xgb_model\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-29 16:03:55,729:INFO:common:yaml file: config/config.yaml loaded successfully]\n",
      "[2023-11-29 16:03:55,731:INFO:common:yaml file: params.yaml loaded successfully]\n",
      "[2023-11-29 16:03:55,732:INFO:common:yaml file: schema.yaml loaded successfully]\n",
      "[2023-11-29 16:03:55,732:INFO:common:created directory at: artifacts]\n",
      "[2023-11-29 16:03:55,733:INFO:common:created directory at: artifacts/data_modelling]\n",
      "[2023-11-29 16:03:55,746:INFO:1934804010:Columns in x: Index(['Airline', 'Source', 'Destination', 'Total_Stops',\n",
      "       'Month_of_Month_of_journey', 'day_of_date_of_journey',\n",
      "       'Duration_minutes'],\n",
      "      dtype='object')]\n",
      "[2023-11-29 16:03:55,747:INFO:1934804010:Airline                      object\n",
      "Source                       object\n",
      "Destination                  object\n",
      "Total_Stops                  object\n",
      "Month_of_Month_of_journey     int64\n",
      "day_of_date_of_journey        int64\n",
      "Duration_minutes              int64\n",
      "dtype: object]\n",
      "[2023-11-29 16:03:55,763:INFO:1934804010:(10682, 31)]\n",
      "[2023-11-29 16:03:55,764:INFO:1934804010:Shape of data is X: (10682, 31),Y:(10682, 1)]\n",
      "[2023-11-29 16:03:55,765:INFO:1934804010:Train Test Split of The Data Started]\n",
      "[2023-11-29 16:03:55,771:INFO:1934804010:Train Test Split Completed. Shapes of training and test data:\n",
      "(8545, 31)\n",
      "(2137, 31)\n",
      "(8545, 1)\n",
      "(2137, 1)]\n",
      "[2023-11-29 16:03:55,771:INFO:1934804010:Training XGBoost Model Started]\n",
      "[2023-11-29 16:03:55,871:INFO:1934804010:Training XGBoost Model Completed]\n",
      "[2023-11-29 16:03:55,873:INFO:1934804010:XGBoost model saved to artifacts/data_modelling/xgb_model.pkl]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saiprakashlikky/anaconda3/envs/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [16:03:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "            config = ConfigurationManager()\n",
    "            data_modelling_config = config.get_modelling_config()\n",
    "            data_modelling = DataModelling(config=data_modelling_config)\n",
    "            x_scaled, y = data_modelling.get_data_transformed_object()\n",
    "            x_train, x_test, y_train, y_test = data_modelling.train_test_variables(x_scaled, y)\n",
    "            xgb_model = data_modelling.model_trainer(x_train, y_train, x_test, y_test)\n",
    "            \n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
